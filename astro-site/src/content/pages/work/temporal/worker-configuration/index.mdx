import Tooltip from '../../../../../components/Tooltip.astro'

# Worker configuration (_Node_)

This is a walkthrough of my experience configuring workers running on Node for [Genesy](https://genesy.ai/).

It focuses on worker configuration, as I found the current (as of Oct 2025) defaults to be misguiding, and want to share my findings, as this is [not only a me-issue](https://temporalio.slack.com/archives/CTRCR8RBP/p1761316809891479).

Readers should understand workers and worker/server interaction before reading this article. ([a good resource](https://temporal.io/blog/building-reliable-distributed-systems-in-node-js-part-2))

_Fun fact_: This article is the reason this site exists.

## Background

<details style="padding: 1rem;">
<summary style="cursor: pointer; margin-bottom: 1rem;">Click to expand; Not necessary to understand the article</summary>

We run two <Tooltip text="Each service simply runs a worker">services </Tooltip>

- **Enrich**: User actions are not processed in the API directly, they are off-loaded to this service, which makes sure that they are processed reliably. I wrote my [CS bachelor's thesis](https://upcommons.upc.edu/entities/publication/d8662ade-c751-4257-bc1f-fb2f8245199d) on why we decided on this, and how I coded the primitives that allow it to work well.
- **Conversation**: Genesy offers [automated campaigns](https://www.genesy.ai/#:~:text=Conditional%20Campaigns), which are likely the most advanced conditional campaign system that exist on any tool. Each campaign has an identity which targets contacts, with which a conversation is started. Each conversation runs as an entity workflow, and identity actions are centralized in a manager workflow.

Even though the learnings apply to both services, most of the investigation comes from the **Conversation** service, as we want it to scale to infinity reliably (started in February with &lt;50k active conversations, in October it handles &gt;500k).

### Why did I start looking into this

Users can see conversations in Genesy via the _inbox_. The inbox also allows sending messages directly.

There's nothing worse for users than sending a message twice. To avoid this, we use [updates](https://docs.temporal.io/encyclopedia/workflow-message-passing) to prompt the manager workflow to send the message, and get a response (whether it was successful or not).

If the manager workflow is not responsive, then the update fails, and the user is sad. As my main job is to make users happy, this had to be solved.

### Priority

_6th of march_, Temporal [announced](https://github.com/temporalio/temporal/issues/1507#issuecomment-2703831058) priority and fairness.

It took 3 months of [me complaining](https://temporalio.slack.com/archives/C08GSTH8WEM/p1758618625126139) that it wasn't working, until priority was an actual useful feature.

I thought that priority would solve this problem. Simply prioritize manager workflows over conversation workflows.
Then we only need to make sure that we can handle _hundreds_ of workflows immediately instead of _hundreds of thousands_.

</details>

## Discovering the problem

I started experimenting with how to make sure `priorityKey=1` workflows are always responsive no matter how many higher priority workflows needed processing. An easy way to stress test it was to limit the number of instances of the service, and thus of workers.

Previously, up to 50 instances were running at a time, as our best guess on making `priorityKey=1` workflows responsive was to process everything instantly. \
I cut down to 10 instances, and at peak times, workers were <Tooltip text="By saturated I mean that for the task queue, the backlog increases: tasksAddRate > tasksDispatchRate">saturated</Tooltip>.

<figure style="margin: 2rem 0; text-align: center;">
    <img
        src="/work/temporal/worker-configuration/stats-by-prioQ.png"
        alt="stats by prioQ"
        style="max-width: 50%;"
    />
    <figcaption style="font-style: italic; margin-top: 0.5rem; color: #666;">
        Stats for the conversation task queue by priority key
    </figcaption>
</figure>

Even though it looks great, as the backlog age is less than one second for `priorityKey=1`, it performed worse. \
That day, we went from ~97% success on users sending messages to ~85%.

At this point, tasks were picked up by workers almost instantly, yet they failed. Why?? \
Looking at metrics, many workflow tasks were failing due to `status_code=not_found`

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/failures-workflowtasklatency.png"
        alt="workflow task failures and latency"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        Workflow task failures and latency
    </figcaption>
</figure>

It's obvious from the image that the failures are related to the <Tooltip text="Time since the worker picks up the task until it responds to the temporal server">task execution latency</Tooltip>. _kappa_ agreed:

```plaintext
metric temporal_request_failure with operation=RespondWorkflowTaskCompleted and
status_code=NOT_FOUND indicates that a worker attempted to report the completion
of a workflow task, but the Temporal service could not find the corresponding
workflow task. This typically happens when the workflow task has already timed
outâ€”meaning the worker did not respond within the default 10-second timeout window
```

The worker was taking more than 10 seconds (default start-to-close timeout) to complete the task. \
This was a surprise, as workflow tasks are lightweight, and should take _tens of milliseconds_ at most.

## Understanding worker configuration

The worker configuration is defined on the `worker.create` method. Here we focus on performance-related options.

### Task execution and polling

Each worker has a number of pollers and slots for each <Tooltip text="workflow/activity/local-activity/nexus">task type</Tooltip>. The pollers grab tasks from the Temporal server, and occupy a slot while the task is being executed.
The reasoning behind having multiple pollers and not just one is due to performance. From the code docs:

```plaintext
In general, a Workflow Worker's performance is mostly network bound (due to
communication latency with the Temporal server). Accepting multiple Workflow
Tasks concurrently helps compensate for network latency, until the point
where the Worker gets CPU bound.
```

There are two ways to configure this: \
The original way, using maxConcurrent<Tooltip text="workflow/activity/local-activity/nexus task">\<taskType></Tooltip>Executions and maxConcurrent<Tooltip text="workflow/activity/local-activity/nexus task">\<taskType></Tooltip>Polls. \
The new way, using the `tuner`, which grants slots until a targetCpu and targetMemory are reached.

_We will focus on workflow and activity tasks, which are the majority of tasks that are executed._

### Threading model

The threading model is explained [here](https://typescript.temporal.io/api/interfaces/worker.WorkerOptions#threading-model). \
Basically, interactions with the Core SDK happen on the main thread (Node's event loop). There's also at least one worker thread, which the main thread delegates workflow tasks to. This is a great idea, as workflow tasks are CPU bound (they run until the next await).

The number of worker threads is configured with the `workflowThreadPoolSize` option. It defaults to 1 if `reuseV8Context` is true, or 2 otherwise.

Workflow tasks tend to be lightweight, so the defaults are usually fine.

### Sticky tasks and worker cache

There's also the `nonStickyToStickyPollRatio`, which indicates the ratio of pollers that will poll from the sticky task queue. \
Along with `maxCachedWorkflows`, it is a very useful optimization to avoid having to replay workflows.

The values should be set according to your workload patterns. \
The docs on it are good, so no need to go deeper here.

## What can we do to fix the problem?

### Task execution and polling

By far the most relevant configuration is related to task execution and polling.

Looking at metrics for number of <Tooltip text="slots are tasks being executed concurrently">slots</Tooltip>, we can see that the number of workflow slots is directly related to the number of tasks failing, and through the previous relationship, the number of workflow tasks timing out.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/failures-slots.png"
        alt="workflow task failures and slots"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        Workflow task failures and slots
    </figcaption>
</figure>

This image is obtained using the `tuner`, targeting 0.8 CPU and 0.6 memory usage, with a workflow task <Tooltip text="time before handling new slots">rampThrottle</Tooltip> of 100ms. Without the rampThrottle, we would get a lot of "out of memory" errors, as the worker would fill its memory trying to execute too many tasks at once.

It's obvious from the image that the tuner is granting too many slots, and that this for some reason, is causing tasks to fail.

The worker takes more than the start-to-close timeout to complete the tasks, so when responding to the Temporal server with a result, the task is no longer found, and the result is discarded. This is very bad, as that same task is reissued, and the process repeats, burning CPU cycles for no reason.

<div style="background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem 1.5rem; margin: 2rem 0; border-radius: 4px;">
    <strong>Key Insight:</strong> Workflow tasks take so long to complete, not because they are expensive
    (they take _tens of milliseconds_), but because there are many slots executing concurrently along with the
    main thread.
</div>

### Threading model

In all my tests, the tuner grants way too many workflow slots no matter what `targetCPU` is specified. To understand why, we need to look at the threading model.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/50+pollers-cpu.png"
        alt="50+ slots CPU usage"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        CPU usage for `targetCPU=0.5` (50+ slots)
    </figcaption>
</figure>

This is running on ECS Fargate, on single vCPU instances.

Running 2 vCPU instances, with the default configuration, the tuner was _slightly more_ useful, but still granted too many slots.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/tuner-2vcpu.png"
        alt="2 vCPUs slots and task failures"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        2 vCPUs slots and task failures
    </figcaption>
</figure>

When configuring a fixed sized workflow task slot supplier, with 2 vCPUs and one worker thread, I could get a bit less than twice the number of slots as with 1 vCPU, but maximum CPU usage was less than 75%, which makes sense, the worker thread was at full capacity, and the main thread was half-idle, mostly due to I/O bound work.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/2vCPU-CPU.png"
        alt="2 vCPUs CPU usage"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        2 vCPUs CPU usage
    </figcaption>
</figure>

## Conclusions

I couldn't get the tuner to work as expected, so I can't recommend using it, it causes too many workflow task slots, which makes them time out and be retried, burning CPU cycles for no reason.

The most important thing is to use a fixed sized workflow task slot supplier, with a size that is a good fit for your workload.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/failures-slots.png"
        alt="workflow task failures and slots"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        Workflow task failures and slots
    </figcaption>
</figure>

If no configuration is used (no tuner, no maxConcurrentWorkflowTaskExecutions), the worker will use the default configuration, which is 40 workflow slots (likely still way too many).

## Why is the default worker configuration so bad?

Well, It's not that bad... \
For parallel programming languages (like go and java, which are likely Temporal's target languages), it makes sense, as we want to reduce the percentage of I/O work, and there's no event loop or main thread that limits performance.

I have explored more in detail the tuner, running on 2vCPU instances, and poller configuration, which you can read about in the [deep-dive](/work/temporal/worker-configuration/deep-dive) article.

## A decent worker configuration

~~We now use~~ A good enough configuration is the following:

```typescript
const config = {
    ... other things
    tuner = {
        workflowTaskSlotSupplier: {
            type: 'fixed-size',
            numSlots: X (5 for conversation, 10 for enrich),
        },
        activityTaskSlotSupplier: {
            type: 'resource-based',
            rampThrottle: '100 ms',
            tunerOptions: {
                targetCpuUsage: 1.0,
                targetMemoryUsage: 0.6,
            },
        },
        // local and nexus tasks don't really matter
    },
    nonStickyToStickyPollRatio: Y (2/5 for conversation, 0.5 for enrich),
    maxCachedWorkflows: Z (150 for conversation, 300 for enrich),
}
```

The difference between conversation and enrich is that conversation workflow tasks are slower, due to replay, and more of the conversation workflows end up not being sticky.

Values for activity tasks are not very relevant, as our workflows don't fan out into many activities, and eager dispatch is enabled. \
Activities are mostly processed instantly, and are very cheap, though they do sit in the event loop, adding a bit of latency to the main thread.

<figure style="margin: 2rem 0;">
    <img
        src="/work/temporal/worker-configuration/saturated-wftask-latency.png"
        alt="workflow tasks latency"
        style="max-width: 100%;"
    />
    <img
        src="/work/temporal/worker-configuration/saturated-wftasks-failures.png"
        alt="workflow tasks failures"
        style="max-width: 100%;"
    />
    <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem; color: #666;">
        Workflow tasks latency and failures with 6 slots
    </figcaption>
</figure>

Running the same load, we can see that with 6 slots, there are no failures, and that the **maximum** time that workflow tasks take to process is close to the start-to-close timeout.
With this configuration, 100% of users sending messages succeed.

### Why not just 1 slot?

Even though the default is not good, the docs are correct in that worker performance is mostly network bound so it's still useful to have multiple slots. We want to make sure that either the workflow thread or the main thread are never idle due to having no work to do.

## Running workers in production

This article is related to worker configuration, but to run workers in production, infrastructure and scaling strategies also need to be considered.

Workers should run on fixed-size instances and scale horizontally. (We use ECS Fargate, and run 1-50 (usually under 10) instances per service, larger teams probably use Kubernetes)

Scaling should be done on average CPU usage across worker instances, and either task backlog count or backlog age. (We send task queue metrics to CloudWatch)

## Further reading

If you are interested in understanding the configuration and the details of the many experiments I have run, you can read the [deep-dive](/work/temporal/worker-configuration/deep-dive) article.
